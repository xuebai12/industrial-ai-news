#!/usr/bin/env python3
"""
è°ƒè¯•è„šæœ¬ï¼šåªè·‘ æŠ“å– â†’ å»é‡ â†’ è¿‡æ»¤ï¼Œè¾“å‡ºæ ‡é¢˜å¯¹æ¯”æŠ¥å‘Šï¼ˆMDæ ¼å¼ï¼‰ã€‚
ä¸æ‰§è¡Œ LLM åˆ†æå’ŒæŠ•é€’ï¼Œå¿«é€ŸéªŒè¯è¿‡æ»¤è§„åˆ™æ•ˆæœã€‚

ç”¨æ³•:
    python debug_filter.py                  # å®Œæ•´æŠ“å–ï¼ˆå«åŠ¨æ€ï¼‰+ å…³é”®è¯è¿‡æ»¤
    python debug_filter.py --skip-dynamic   # è·³è¿‡ Playwright åŠ¨æ€æŠ“å–
    python debug_filter.py --skip-llm       # åªç”¨å…³é”®è¯ï¼ˆä¸è°ƒ LLM Cloudï¼‰
    python debug_filter.py --max 10         # æ¯æºæœ€å¤šæŠ“ 10 ç¯‡ï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰
"""
from __future__ import annotations

import argparse
import logging
import os
import sys
from datetime import date

logging.basicConfig(
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


def parse_args():
    p = argparse.ArgumentParser(description="Filter debug: scrape â†’ dedupe â†’ filter, output MD")
    p.add_argument("--skip-dynamic", action="store_true", help="è·³è¿‡ Playwright åŠ¨æ€æŠ“å–")
    p.add_argument("--skip-llm", action="store_true", help="åªç”¨å…³é”®è¯è¿‡æ»¤ï¼Œä¸è°ƒ LLM Cloud")
    p.add_argument("--max", type=int, default=20, dest="max_articles", help="æ¯æºæœ€å¤šæŠ“å–æ¡æ•°ï¼ˆé»˜è®¤ 20ï¼‰")
    p.add_argument("--output-dir", default="output", help="MD æ–‡ä»¶è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ outputï¼‰")
    return p.parse_args()


def main():
    args = parse_args()
    os.makedirs(args.output_dir, exist_ok=True)
    today = date.today().strftime("%Y-%m-%d")

    # â”€â”€ 1. æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from config import DATA_SOURCES
    from src.scrapers.rss_scraper import scrape_rss
    from src.scrapers.web_scraper import scrape_web_sources

    YOUTUBE_MAX = int(os.getenv("YOUTUBE_MAX_ITEMS", "5"))
    all_articles = []

    logger.info("=== [SCRAPE] RSS sources ===")
    for source in [s for s in DATA_SOURCES if s.source_type == "rss"]:
        limit = min(args.max_articles, YOUTUBE_MAX) if source.name.lower().startswith("youtube rss:") else args.max_articles
        try:
            arts = scrape_rss(name=source.name, url=source.url,
                              language=source.language, category=source.category,
                              max_items=limit)
            all_articles.extend(arts)
            logger.info("  [RSS] %s  â†’ %d articles", source.name, len(arts))
        except Exception as e:
            logger.warning("  [RSS] %s failed: %s", source.name, e)

    logger.info("=== [SCRAPE] Web sources ===")
    try:
        web = scrape_web_sources(args.max_articles)
        all_articles.extend(web)
        logger.info("  [WEB] %d articles", len(web))
    except Exception as e:
        logger.warning("  [WEB] failed: %s", e)

    if not args.skip_dynamic:
        logger.info("=== [SCRAPE] Dynamic sources (Playwright) ===")
        try:
            from src.scrapers.dynamic_scraper import scrape_dynamic_sources
            dyn = scrape_dynamic_sources(args.max_articles)
            all_articles.extend(dyn)
            logger.info("  [DYN] %d articles", len(dyn))
        except Exception as e:
            logger.warning("  [DYN] failed: %s", e)
    else:
        logger.info("[SCRAPE] Skipping dynamic scrapers (--skip-dynamic)")

    # â”€â”€ 2. å»é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from urllib.parse import parse_qsl, urlencode, urlparse, urlunparse

    def _norm_url(url: str) -> str:
        if not url:
            return ""
        p = urlparse(url.strip())
        netloc = p.netloc.lower().rstrip(":80").rstrip(":443")
        q = urlencode(sorted(parse_qsl(p.query, keep_blank_values=True)))
        path = p.path.rstrip("/") or "/"
        return urlunparse((p.scheme.lower(), netloc, path, p.params, q, ""))

    seen: set[str] = set()
    deduped = []
    for a in all_articles:
        key = _norm_url(getattr(a, "url", "") or "")
        if not key:
            key = f"{getattr(a, 'source', '')}:{getattr(a, 'title', '')}"
        if key not in seen:
            seen.add(key)
            deduped.append(a)

    logger.info("[DEDUPE] %d â†’ %d", len(all_articles), len(deduped))

    # â”€â”€ 3. å…³é”®è¯è¯„åˆ†ï¼ˆä¸è°ƒ LLMï¼Œå…ˆçœ‹å…³é”®è¯å±‚ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from src.filters.ollama_filter import keyword_score, RELEVANCE_THRESHOLD

    scored_pass, scored_fail = [], []
    for a in deduped:
        score, _ = keyword_score(a)
        a.relevance_score = score
        (scored_pass if score >= RELEVANCE_THRESHOLD else scored_fail).append(a)

    logger.info("[KW-FILTER] pass=%d  fail=%d  threshold=%d",
                len(scored_pass), len(scored_fail), RELEVANCE_THRESHOLD)

    # â”€â”€ 4. LLM Cloud è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not args.skip_llm:
        from src.filters.ollama_filter import filter_articles
        final_pass = filter_articles(deduped, skip_llm=False)
        llm_note = "ï¼ˆå…³é”®è¯ + LLM åŒé‡è¿‡æ»¤ï¼‰"
    else:
        final_pass = sorted(scored_pass, key=lambda a: a.relevance_score, reverse=True)
        llm_note = "ï¼ˆä»…å…³é”®è¯è¿‡æ»¤ï¼Œå·²è·³è¿‡ LLMï¼‰"

    # â”€â”€ 5. è¾“å‡º MD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines = [
        f"# è¿‡æ»¤è°ƒè¯•æŠ¥å‘Š {today}",
        "",
        f"> æŠ“å– {len(all_articles)} â†’ å»é‡å {len(deduped)} â†’ å…³é”®è¯é€šè¿‡ {len(scored_pass)} â†’ æœ€ç»ˆé€šè¿‡ {len(final_pass)} {llm_note}",
        "",
        "---",
        "",
        f"## âœ… é€šè¿‡è¿‡æ»¤çš„æ–°é—»ï¼ˆ{len(final_pass)} æ¡ï¼‰",
        "",
    ]
    for i, a in enumerate(final_pass, 1):
        title = getattr(a, "title", "").strip() or "(æ— æ ‡é¢˜)"
        cat = getattr(a, "category", "")
        score = getattr(a, "relevance_score", "?")
        url = getattr(a, "url", "") or ""
        lines.append(f"{i:02d}. [{cat}] {title} _(score={score})_  \n    {url}")

    lines += [
        "",
        "---",
        "",
        f"## âŒ è¢«è¿‡æ»¤æ‰çš„æ–°é—»ï¼ˆ{len(scored_fail)} æ¡å…³é”®è¯=0ï¼‰",
        "",
    ]
    for i, a in enumerate(sorted(scored_fail, key=lambda x: getattr(x, "title", "")), 1):
        title = getattr(a, "title", "").strip() or "(æ— æ ‡é¢˜)"
        cat = getattr(a, "category", "")
        url = getattr(a, "url", "") or ""
        lines.append(f"{i:02d}. [{cat}] {title}  \n    {url}")

    out_path = os.path.join(args.output_dir, f"debug-filter-{today}.md")
    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines) + "\n")

    logger.info("[DONE] æŠ¥å‘Šå·²å†™å…¥: %s", out_path)
    print(f"\nğŸ“„ æŠ¥å‘Šè·¯å¾„: {out_path}")
    return 0


if __name__ == "__main__":
    sys.exit(main())
